{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV Loaded Successfully! Number of rows: 32780\n",
      "   Unnamed: 0                                               name  grape  \\\n",
      "0           0  1000 Stories Bourbon Barrel Aged Batch Blue Ca...    NaN   \n",
      "1           1  1000 Stories Bourbon Barrel Aged Gold Rush Red...    NaN   \n",
      "2           2  1000 Stories Bourbon Barrel Aged Gold Rush Red...    NaN   \n",
      "3           3    1000 Stories Bourbon Barrel Aged Zinfandel 2013    NaN   \n",
      "4           4    1000 Stories Bourbon Barrel Aged Zinfandel 2014    NaN   \n",
      "\n",
      "                    region   variety  rating  \\\n",
      "0    Mendocino, California  Red Wine    91.0   \n",
      "1               California  Red Wine    89.0   \n",
      "2               California  Red Wine    90.0   \n",
      "3  North Coast, California  Red Wine    91.0   \n",
      "4               California  Red Wine    90.0   \n",
      "\n",
      "                                               notes  \n",
      "0  This is a very special, limited release of 100...  \n",
      "1  The California Gold Rush was a period of coura...  \n",
      "2  The California Gold Rush was a period of coura...  \n",
      "3  The wine has a deep, rich purple color. An int...  \n",
      "4  Batch #004 is the first release of the 2014 vi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"../wine-ratings.csv\"\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(f\"‚ùå ERROR: CSV file not found at {CSV_PATH}\")\n",
    "else:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"‚úÖ CSV Loaded Successfully! Number of rows: {len(df)}\")\n",
    "    print(df.head())  # Show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Deleting old FAISS index...\n",
      "üìå Creating a new FAISS index...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     21\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(row[column_name])) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m     23\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS(\n\u001b[0;32m     24\u001b[0m     embeddings,\n\u001b[0;32m     25\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;241m768\u001b[39m),\n\u001b[0;32m     26\u001b[0m     InMemoryDocstore({}),\n\u001b[0;32m     27\u001b[0m     index_to_docstore_id\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(DB_PATH)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ FAISS rebuilt with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pcsal\\OneDrive\\Desktop\\Coursera Courses\\Generative AI - Intro\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:286\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    285\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m )\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\pcsal\\OneDrive\\Desktop\\Coursera Courses\\Generative AI - Intro\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:341\u001b[0m, in \u001b[0;36mFAISS.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    340\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_documents(texts)\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pcsal\\OneDrive\\Desktop\\Coursera Courses\\Generative AI - Intro\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:313\u001b[0m, in \u001b[0;36mFAISS.__add\u001b[1;34m(self, texts, embeddings, metadatas, ids)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    312\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# Add information to docstore and index.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd({id_: doc \u001b[38;5;28;01mfor\u001b[39;00m id_, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, documents)})\n",
      "File \u001b[1;32mc:\\Users\\pcsal\\OneDrive\\Desktop\\Coursera Courses\\Generative AI - Intro\\.venv\\Lib\\site-packages\\faiss\\class_wrappers.py:228\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03mThe index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03mThe vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    `dtype` must be float32.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "DB_PATH = \"faiss_index\"\n",
    "CSV_PATH = \"../wine-ratings.csv\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete FAISS index if it exists\n",
    "if os.path.exists(DB_PATH):\n",
    "    print(\"üõë Deleting old FAISS index...\")\n",
    "    os.system(f\"rm -rf {DB_PATH}\")\n",
    "\n",
    "print(\"üìå Creating a new FAISS index...\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "column_name = \"rating\" if \"rating\" in df.columns else df.columns[1]\n",
    "docs = [Document(page_content=str(row[column_name])) for _, row in df.iterrows()]\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embeddings,\n",
    "    faiss.IndexFlatL2(768),\n",
    "    InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "vector_store.add_documents(docs)\n",
    "vector_store.save_local(DB_PATH)\n",
    "\n",
    "print(f\"‚úÖ FAISS rebuilt with {len(docs)} documents!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Check the embedding dimension\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "print(f\"‚úÖ Embedding Dimension: {len(test_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = len(embeddings.embed_query(\"test\"))  # Get correct dimension dynamically\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embeddings,\n",
    "    faiss.IndexFlatL2(EMBED_DIM),  # Correct dimension\n",
    "    InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS can now accept documents!\n"
     ]
    }
   ],
   "source": [
    "sample_doc = [Document(page_content=\"Test document\")]\n",
    "vector_store.add_documents(sample_doc)\n",
    "print(\"‚úÖ FAISS can now accept documents!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Deleting old FAISS index...\n",
      "üìå Rebuilding FAISS with correct dimension...\n",
      "‚úÖ FAISS rebuilt with 32780 documents!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "DB_PATH = \"faiss_index\"\n",
    "if os.path.exists(DB_PATH):\n",
    "    print(\"üõë Deleting old FAISS index...\")\n",
    "    shutil.rmtree(DB_PATH)\n",
    "\n",
    "print(\"üìå Rebuilding FAISS with correct dimension...\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "column_name = \"rating\" if \"rating\" in df.columns else df.columns[1]\n",
    "docs = [Document(page_content=str(row[column_name])) for _, row in df.iterrows()]\n",
    "\n",
    "EMBED_DIM = len(embeddings.embed_query(\"test\"))  # Dynamically get correct dimension\n",
    "vector_store = FAISS(\n",
    "    embeddings,\n",
    "    faiss.IndexFlatL2(EMBED_DIM),\n",
    "    InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "vector_store.add_documents(docs)\n",
    "vector_store.save_local(DB_PATH)\n",
    "print(f\"‚úÖ FAISS rebuilt with {len(docs)} documents!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Loading existing FAISS index...\n",
      "üìå FAISS index contains 32780 vectors.\n",
      "üìÑ Sample stored documents from FAISS:\n",
      "üìú Document 1: Name: Fairview Pinotage 2005. Grape: nan. Region: South Africa. Variety: Red Wine. Rating: 88.0. Notes: Colour: Vibrant purple red..\n",
      "üìú Document 2: Name: Fairview Viognier 2009. Grape: nan. Region: South Africa. Variety: White Wine. Rating: 89.0. Notes: White pear and red apple fruit aromas, with a lovely spiciness. Fragrant marmalade and pear fr\n",
      "üìú Document 3: Name: Fairview Viognier 2007. Grape: nan. Region: South Africa. Variety: White Wine. Rating: 90.0. Notes: Fruity-floral notes of pears, apricots and rose petals with whiffs of lavender. The 2007 Viogn\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DB_PATH):\n",
    "    print(\"üìå Loading existing FAISS index...\")\n",
    "    vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(f\"üìå FAISS index contains {vector_store.index.ntotal} vectors.\")\n",
    "\n",
    "    # üîç DEBUG: Print sample stored documents\n",
    "    print(\"üìÑ Sample stored documents from FAISS:\")\n",
    "    sample_docs = vector_store.similarity_search(\"random\", k=3)\n",
    "    for i, doc in enumerate(sample_docs):\n",
    "        print(f\"üìú Document {i+1}: {doc.page_content[:200]}\")  # Print first 200 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Response from LLaMA_CPP:\n",
      "{'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Argentina is known for producing high-quality wines, and it can be challenging to recommend just one. However, some of the most famous and highly regarded wines from Argentina include:\\n\\n1. Malbec: This grape', 'role': 'assistant'}}], 'created': 1739651709, 'id': 'chatcmpl-2bQ2QXnk3CvCD9S2cAAn46vyYYEnYl34', 'model': 'LLaMA_CPP', 'object': 'chat.completion', 'usage': {'completion_tokens': 50, 'prompt_tokens': 64, 'total_tokens': 114}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# LLaMA_CPP API URL (Check if it's running)\n",
    "LLAMA_API_URL = \"http://127.0.0.1:8080/v1/chat/completions\"\n",
    "\n",
    "# Define a valid OpenAI-compatible request\n",
    "data = {\n",
    "    \"model\": \"LLaMA_CPP\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a sommelier helping users find the best wines.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the best wine from America?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 50,  # ‚úÖ Reduce tokens for a faster response\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Send the request\n",
    "try:\n",
    "    response = requests.post(LLAMA_API_URL, json=data)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "    # Print the response\n",
    "    print(\"‚úÖ Success! Response from LLaMA_CPP:\")\n",
    "    print(response.json())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"‚ùå Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
